{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "mask = torch.tensor([True, False, True])  # shape (3,)\n",
    "x.masked_fill(mask, 0)  # ❌ 错，因为 (3,) 无法广播到 (2, 3)\n",
    "\n",
    "a = torch.rand((3, 5))\n",
    "print(a)\n",
    "print(a.unsqueeze(1))\n",
    "print(a.unsqueeze(1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "N, num_head, q_seq_len, k_seq_len = 1, 2, 5, 5\n",
    "mul_res = torch.tensor([[\n",
    "[\n",
    "    [1.2, 0.3, 0.4, 0.2, 0.1],  # token 0\n",
    "    [0.3, 1.0, 0.6, 0.5, 0.1],  # token 1\n",
    "    [0.4, 0.6, 1.3, 0.7, 0.2],  # token 2\n",
    "    [0.2, 0.5, 0.7, 1.1, 0.4],  # token 3\n",
    "    [0.1, 0.1, 0.2, 0.4, 1.5]   # token 4 (padding)\n",
    "],\n",
    "[\n",
    "    [0.4, 0.6, 1.3, 0.7, 0.2],  \n",
    "    [0.2, 0.5, 0.7, 1.1, 0.4],  \n",
    "    [0.1, 0.1, 0.2, 0.4, 1.5],\n",
    "    [1.2, 0.3, 0.4, 0.2, 0.1],  \n",
    "    [0.3, 1.0, 0.6, 0.5, 0.1],  \n",
    "]\n",
    "]])\n",
    "# print(mul_res.shape)    # (N, num_head, q_len, k_len)\n",
    "key_padding_mask = torch.tensor([[False, False, False, False, True]])  # 只有最后一个 token 是 padding\n",
    "# print(key_padding_mask)\n",
    "attn_mask = torch.triu(torch.ones((q_seq_len, k_seq_len)), diagonal=1).bool()   # (q_len=5, k_len=5)\n",
    "\n",
    "if key_padding_mask is not None:\n",
    "    key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "    key_padding_mask = key_padding_mask.expand((N, num_head, q_seq_len, k_seq_len))\n",
    "    mul_res = torch.masked_fill(mul_res, key_padding_mask, -torch.inf)\n",
    "print(mul_res.shape)\n",
    "print(mul_res)\n",
    "if attn_mask is not None:\n",
    "    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "    attn_mask = attn_mask.expand((N, num_head, q_seq_len, k_seq_len))\n",
    "    mul_res = torch.masked_fill(mul_res, attn_mask, -torch.inf)\n",
    "print(mul_res.shape)\n",
    "print(mul_res)\n",
    "\n",
    "# key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "# key_padding_mask = key_padding_mask.expand((1, num_head, 5, 5))\n",
    "# print(key_padding_mask)\n",
    "# print(torch.softmax(torch.tensor([float(\"inf\"), float(\"inf\")]), dim=0))\n",
    "# attn_output_weights = attn_output_weights.masked_fill(, float('-inf'))\n",
    "# print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from transformers import AutoModel\n",
    "from transformers import BertModel\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "bert = AutoModel.from_pretrained(ckp)\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8358)\n",
      "tensor(0.8358)\n",
      "tensor(0.8265)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.randn((2, 3))\n",
    "inputs = torch.softmax(inputs, dim=0)\n",
    "labels = torch.tensor([0, 2])\n",
    "print(nn.CrossEntropyLoss()(inputs, labels))\n",
    "print(F.cross_entropy(inputs, labels))\n",
    "print(F.cross_entropy(inputs[0], labels[0]))\n",
    "\n",
    "print(inputs[:, 0:2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# t = torch.arange(24).view(2, 3, 4)\n",
    "# print(t)\n",
    "# t = t.view(2, 4, 3)\n",
    "# print(t)\n",
    "\n",
    "\n",
    "a = torch.arange(12)\n",
    "print(torch.stack([a] * 3, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
