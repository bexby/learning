{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "mask = torch.tensor([True, False, True])  # shape (3,)\n",
    "x.masked_fill(mask, 0)  # ❌ 错，因为 (3,) 无法广播到 (2, 3)\n",
    "\n",
    "a = torch.rand((3, 5))\n",
    "print(a)\n",
    "print(a.unsqueeze(1))\n",
    "print(a.unsqueeze(1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "N, num_head, q_seq_len, k_seq_len = 1, 2, 5, 5\n",
    "mul_res = torch.tensor([[\n",
    "[\n",
    "    [1.2, 0.3, 0.4, 0.2, 0.1],  # token 0\n",
    "    [0.3, 1.0, 0.6, 0.5, 0.1],  # token 1\n",
    "    [0.4, 0.6, 1.3, 0.7, 0.2],  # token 2\n",
    "    [0.2, 0.5, 0.7, 1.1, 0.4],  # token 3\n",
    "    [0.1, 0.1, 0.2, 0.4, 1.5]   # token 4 (padding)\n",
    "],\n",
    "[\n",
    "    [0.4, 0.6, 1.3, 0.7, 0.2],  \n",
    "    [0.2, 0.5, 0.7, 1.1, 0.4],  \n",
    "    [0.1, 0.1, 0.2, 0.4, 1.5],\n",
    "    [1.2, 0.3, 0.4, 0.2, 0.1],  \n",
    "    [0.3, 1.0, 0.6, 0.5, 0.1],  \n",
    "]\n",
    "]])\n",
    "# print(mul_res.shape)    # (N, num_head, q_len, k_len)\n",
    "key_padding_mask = torch.tensor([[False, False, False, False, True]])  # 只有最后一个 token 是 padding\n",
    "# print(key_padding_mask)\n",
    "attn_mask = torch.triu(torch.ones((q_seq_len, k_seq_len)), diagonal=1).bool()   # (q_len=5, k_len=5)\n",
    "\n",
    "if key_padding_mask is not None:\n",
    "    key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "    key_padding_mask = key_padding_mask.expand((N, num_head, q_seq_len, k_seq_len))\n",
    "    mul_res = torch.masked_fill(mul_res, key_padding_mask, -torch.inf)\n",
    "print(mul_res.shape)\n",
    "print(mul_res)\n",
    "if attn_mask is not None:\n",
    "    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "    attn_mask = attn_mask.expand((N, num_head, q_seq_len, k_seq_len))\n",
    "    mul_res = torch.masked_fill(mul_res, attn_mask, -torch.inf)\n",
    "print(mul_res.shape)\n",
    "print(mul_res)\n",
    "\n",
    "# key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "# key_padding_mask = key_padding_mask.expand((1, num_head, 5, 5))\n",
    "# print(key_padding_mask)\n",
    "# print(torch.softmax(torch.tensor([float(\"inf\"), float(\"inf\")]), dim=0))\n",
    "# attn_output_weights = attn_output_weights.masked_fill(, float('-inf'))\n",
    "# print(attn_output_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from transformers import AutoModel\n",
    "from transformers import BertModel\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "bert = AutoModel.from_pretrained(ckp)\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8358)\n",
      "tensor(0.8358)\n",
      "tensor(0.8265)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = torch.randn((2, 3))\n",
    "inputs = torch.softmax(inputs, dim=0)\n",
    "labels = torch.tensor([0, 2])\n",
    "print(nn.CrossEntropyLoss()(inputs, labels))\n",
    "print(F.cross_entropy(inputs, labels))\n",
    "print(F.cross_entropy(inputs[0], labels[0]))\n",
    "\n",
    "print(inputs[:, 0:2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# t = torch.arange(24).view(2, 3, 4)\n",
    "# print(t)\n",
    "# t = t.view(2, 4, 3)\n",
    "# print(t)\n",
    "\n",
    "\n",
    "a = torch.arange(12)\n",
    "print(torch.stack([a] * 3, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res_values, res_indices\n\u001b[0;32m     22\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m v, i \u001b[38;5;241m=\u001b[39m \u001b[43mtop_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m, in \u001b[0;36mtop_p\u001b[1;34m(data, p)\u001b[0m\n\u001b[0;32m      4\u001b[0m max_k \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m top_k, indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# top_k has been sorted by descending order\u001b[39;00m\n\u001b[0;32m      7\u001b[0m cumsum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcumsum(top_k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(cumsum[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m p)\u001b[38;5;241m.\u001b[39mitem():     \u001b[38;5;66;03m# if k is not large enough to cover cumulated probability p \u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def top_p(data: torch.Tensor, p):\n",
    "    max_k = data.shape[-1]\n",
    "    k = 2000\n",
    "    top_k, indices = torch.topk(data, k)    # top_k has been sorted by descending order\n",
    "    cumsum = torch.cumsum(top_k, dim=-1)\n",
    "    if not torch.all(cumsum[:, -1] > p).item():     # if k is not large enough to cover cumulated probability p \n",
    "        top_k, indices = torch.topk(data, max_k)\n",
    "        cumsum = torch.cumsum(top_k, dim=-1)\n",
    "    sorted_cum_topk, sorted_cum_indices = torch.sort(cumsum, dim=-1, descending=True) \n",
    "    targets = torch.searchsorted(sorted_cum_topk, p)\n",
    "    bs, max_len = sorted_cum_topk.shape\n",
    "    positions = torch.arange(max_len).unsqueeze(0).expand(bs, -1)\n",
    "    top_k = top_k.masked_fill(positions > targets, 0)\n",
    "    samples_idx = torch.multinomial(top_k, 1)\n",
    "    res_indices = torch.gather(indices, -1, samples_idx)\n",
    "    res_values = torch.gather(data, -1, res_indices)\n",
    "    return res_values, res_indices\n",
    "\n",
    "\n",
    "t = torch.randn((2, 10))\n",
    "v, i = top_p(t, 0.6)\n",
    "# print(t)\n",
    "# print(v)\n",
    "# print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('in_proj_weight', tensor([[ 0.0426, -0.0327,  0.0439,  ..., -0.0429, -0.0016,  0.0211],\n",
      "        [-0.0132, -0.0017,  0.0009,  ...,  0.0218, -0.0096,  0.0073],\n",
      "        [ 0.0427, -0.0374, -0.0036,  ..., -0.0209,  0.0304, -0.0140],\n",
      "        ...,\n",
      "        [-0.0309, -0.0217,  0.0379,  ...,  0.0281,  0.0344, -0.0168],\n",
      "        [ 0.0157,  0.0332,  0.0440,  ..., -0.0429,  0.0207,  0.0346],\n",
      "        [-0.0140, -0.0269,  0.0143,  ..., -0.0124, -0.0055, -0.0067]])), ('in_proj_bias', tensor([0., 0., 0.,  ..., 0., 0., 0.])), ('out_proj.weight', tensor([[-0.0180,  0.0349, -0.0099,  ..., -0.0081,  0.0224,  0.0336],\n",
      "        [-0.0252,  0.0219,  0.0020,  ..., -0.0066, -0.0307,  0.0253],\n",
      "        [-0.0333, -0.0226,  0.0165,  ...,  0.0206, -0.0189,  0.0132],\n",
      "        ...,\n",
      "        [ 0.0216, -0.0233, -0.0103,  ..., -0.0149,  0.0137,  0.0055],\n",
      "        [-0.0287,  0.0069, -0.0328,  ..., -0.0105, -0.0038, -0.0154],\n",
      "        [ 0.0281, -0.0262,  0.0110,  ..., -0.0336,  0.0359,  0.0253]])), ('out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from MyTransformers import MyMultiHeadAttention\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "input = torch.randn((4, 5, 764))\n",
    "tm = MultiheadAttention(768, 12, batch_first=True)\n",
    "print(tm.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF wte shape: (50257, 768)\n",
      "My wte shape: (50257, 768)\n",
      "HF wpe shape: (1024, 768)\n",
      "My wpe shape: (1024, 768)\n",
      "After copy: wte mean_abs: 0.0 max: 0.0\n",
      "After copy: wpe mean_abs: 0.0 max: 0.0\n",
      "wte sample diff [0,:8]: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "wpe sample diff [0,:8]: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# fix_copy_embeddings.py\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from gpt2 import GPT2, GPT2Config   # 根据你的路径调整导入\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "hf = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE).eval()\n",
    "cfg = hf.config\n",
    "\n",
    "# instantiate your model (same config)\n",
    "mycfg = GPT2Config(hidden_size=cfg.n_embd, vocab_size=cfg.vocab_size,\n",
    "                   num_hidden_layer=cfg.n_layer, num_head=cfg.n_head, max_len=cfg.n_positions)\n",
    "my = GPT2(mycfg).to(DEVICE).eval()\n",
    "\n",
    "# make sure embedding parameter shapes match\n",
    "print(\"HF wte shape:\", tuple(hf.transformer.wte.weight.shape))\n",
    "print(\"My wte shape:\", tuple(my.embedding.word_embedding.weight.shape))\n",
    "print(\"HF wpe shape:\", tuple(hf.transformer.wpe.weight.shape))\n",
    "print(\"My wpe shape:\", tuple(my.embedding.position_embedding.weight.shape))\n",
    "\n",
    "# explicit copy\n",
    "with torch.no_grad():\n",
    "    # copy token embedding\n",
    "    my.embedding.word_embedding.weight.data.copy_(hf.transformer.wte.weight.data.to(my.embedding.word_embedding.weight.device))\n",
    "    # copy position embedding\n",
    "    my.embedding.position_embedding.weight.data.copy_(hf.transformer.wpe.weight.data.to(my.embedding.position_embedding.weight.device))\n",
    "\n",
    "# tie lm_head to embedding if you expect that (do this AFTER copying if you reassign)\n",
    "try:\n",
    "    # If your model ties at init, this is probably already the case. If not, set it here:\n",
    "    my.lm_head.weight = my.embedding.word_embedding.weight\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# verify exact equality (should be zero)\n",
    "diff_wte = (my.embedding.word_embedding.weight.detach().cpu() - hf.transformer.wte.weight.detach().cpu()).abs()\n",
    "diff_wpe = (my.embedding.position_embedding.weight.detach().cpu() - hf.transformer.wpe.weight.detach().cpu()).abs()\n",
    "print(\"After copy: wte mean_abs:\", float(diff_wte.mean().item()), \"max:\", float(diff_wte.max().item()))\n",
    "print(\"After copy: wpe mean_abs:\", float(diff_wpe.mean().item()), \"max:\", float(diff_wpe.max().item()))\n",
    "\n",
    "# quick element samples to sanity-check\n",
    "print(\"wte sample diff [0,:8]:\", diff_wte[0, :8])\n",
    "print(\"wpe sample diff [0,:8]:\", diff_wpe[0, :8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying HF weights to my model (robust) ...\n",
      "Copy finished.\n",
      "\n",
      "Checking per-layer parameter equality (q/k/v, c_proj, mlp) ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3072) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m chk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.c_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, hfsd[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.attn.c_proj.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m], my\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# mlp\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[43mchk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.c_fc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhfsd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer.h.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.mlp.c_fc.weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m chk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.c_proj_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, hfsd[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mlp.c_proj.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m], my\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mfnn\u001b[38;5;241m.\u001b[39mlinear2\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n",
      "Cell \u001b[1;32mIn[4], line 124\u001b[0m, in \u001b[0;36mchk\u001b[1;34m(name, hf_tensor, my_tensor)\u001b[0m\n\u001b[0;32m    122\u001b[0m hf \u001b[38;5;241m=\u001b[39m hf_tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    123\u001b[0m myt \u001b[38;5;241m=\u001b[39m my_tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m--> 124\u001b[0m m \u001b[38;5;241m=\u001b[39m (\u001b[43mhf\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmyt\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    126\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend((name, m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3072) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# deep_stack_debug.py\n",
    "import torch, math, sys\n",
    "from transformers import GPT2LMHeadModel\n",
    "from gpt2 import GPT2, GPT2Config  # 根据你的文件名调整 import\n",
    "torch.set_printoptions(sci_mode=False, linewidth=200)\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "HF_NAME = \"gpt2\"\n",
    "TEXT = \"Hello, my dog is cute\"\n",
    "\n",
    "def mae(a,b): return (a.detach().float()-b.detach().float()).abs().mean().item()\n",
    "def maestat(a,b):\n",
    "    a=a.detach().cpu().float(); b=b.detach().cpu().float()\n",
    "    d=(a-b).abs()\n",
    "    return {\"mean_abs\":float(d.mean().item()), \"max_abs\":float(d.max().item()), \"shape\":tuple(a.shape)}\n",
    "\n",
    "# ---------- load models ----------\n",
    "hf = GPT2LMHeadModel.from_pretrained(HF_NAME).to(DEVICE).eval()\n",
    "cfg = hf.config\n",
    "mycfg = GPT2Config(hidden_size=cfg.n_embd, vocab_size=cfg.vocab_size,\n",
    "                   num_hidden_layer=cfg.n_layer, num_head=cfg.n_head, max_len=cfg.n_positions)\n",
    "my = GPT2(mycfg).to(DEVICE).eval()\n",
    "\n",
    "# ---------- robust copy of core weights (embeddings + per-layer) ----------\n",
    "print(\"Copying HF weights to my model (robust) ...\")\n",
    "hfsd = hf.state_dict()\n",
    "with torch.no_grad():\n",
    "    # embeddings\n",
    "    my.embedding.word_embedding.weight.data.copy_(hfsd[\"transformer.wte.weight\"].to(my.embedding.word_embedding.weight.device))\n",
    "    my.embedding.position_embedding.weight.data.copy_(hfsd[\"transformer.wpe.weight\"].to(my.embedding.position_embedding.weight.device))\n",
    "    # tie lm_head if present\n",
    "    try:\n",
    "        my.lm_head.weight = my.embedding.word_embedding.weight\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # per-layer\n",
    "    for i in range(cfg.n_layer):\n",
    "        # layernorms\n",
    "        copy_map = [\n",
    "            (f\"transformer.h.{i}.ln_1.weight\", my.layers[i].layernorm1.weight),\n",
    "            (f\"transformer.h.{i}.ln_1.bias\",   my.layers[i].layernorm1.bias),\n",
    "            (f\"transformer.h.{i}.ln_2.weight\", my.layers[i].layernorm2.weight),\n",
    "            (f\"transformer.h.{i}.ln_2.bias\",   my.layers[i].layernorm2.bias),\n",
    "        ]\n",
    "        for k,dst in copy_map:\n",
    "            if k in hfsd:\n",
    "                dst.data.copy_(hfsd[k].to(dst.device))\n",
    "\n",
    "        # c_attn (qkv) : canonicalize and split\n",
    "        W = hfsd[f\"transformer.h.{i}.attn.c_attn.weight\"]\n",
    "        b = hfsd[f\"transformer.h.{i}.attn.c_attn.bias\"]\n",
    "        d_model = cfg.n_embd\n",
    "        if W.shape[0] == 3*d_model and W.shape[1]==d_model:\n",
    "            Wcanon = W\n",
    "        elif W.shape[0] == d_model and W.shape[1] == 3*d_model:\n",
    "            Wcanon = W.t().contiguous()\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected c_attn.weight shape: {W.shape}\")\n",
    "        Wq,Wk,Wv = Wcanon.split(d_model, dim=0)\n",
    "        bq,bk,bv = b.split(d_model, dim=0)\n",
    "        my.layers[i].self_attn.q_w.weight.data.copy_(Wq.to(my.layers[i].self_attn.q_w.weight.device))\n",
    "        my.layers[i].self_attn.q_w.bias.data.copy_(bq.to(my.layers[i].self_attn.q_w.bias.device))\n",
    "        my.layers[i].self_attn.k_w.weight.data.copy_(Wk.to(my.layers[i].self_attn.k_w.weight.device))\n",
    "        my.layers[i].self_attn.k_w.bias.data.copy_(bk.to(my.layers[i].self_attn.k_w.bias.device))\n",
    "        my.layers[i].self_attn.v_w.weight.data.copy_(Wv.to(my.layers[i].self_attn.v_w.weight.device))\n",
    "        my.layers[i].self_attn.v_w.bias.data.copy_(bv.to(my.layers[i].self_attn.v_w.bias.device))\n",
    "\n",
    "        # attn out proj (c_proj)\n",
    "        Wcproj = hfsd[f\"transformer.h.{i}.attn.c_proj.weight\"]\n",
    "        bcproj = hfsd[f\"transformer.h.{i}.attn.c_proj.bias\"]\n",
    "        # copy orientation-robustly\n",
    "        dstW = my.layers[i].self_attn.linear.weight\n",
    "        if tuple(Wcproj.shape) == tuple(dstW.shape):\n",
    "            dstW.data.copy_(Wcproj.to(dstW.device))\n",
    "        elif tuple(Wcproj.t().contiguous().shape) == tuple(dstW.shape):\n",
    "            dstW.data.copy_(Wcproj.t().contiguous().to(dstW.device))\n",
    "        else:\n",
    "            raise RuntimeError(f\"unexpected c_proj shape {Wcproj.shape} vs {dstW.shape}\")\n",
    "        my.layers[i].self_attn.linear.bias.data.copy_(bcproj.to(my.layers[i].self_attn.linear.bias.device))\n",
    "\n",
    "        # mlp: c_fc -> linear1 (HF often (d,4d)), c_proj -> linear2 (HF often (4d,d))\n",
    "        W_fc = hfsd[f\"transformer.h.{i}.mlp.c_fc.weight\"]\n",
    "        b_fc = hfsd[f\"transformer.h.{i}.mlp.c_fc.bias\"]\n",
    "        dst_fc = my.layers[i].fnn.linear1.weight\n",
    "        # try direct or transpose\n",
    "        if tuple(W_fc.shape) == tuple(dst_fc.shape):\n",
    "            dst_fc.data.copy_(W_fc.to(dst_fc.device))\n",
    "        elif tuple(W_fc.t().contiguous().shape) == tuple(dst_fc.shape):\n",
    "            dst_fc.data.copy_(W_fc.t().contiguous().to(dst_fc.device))\n",
    "        else:\n",
    "            raise RuntimeError(f\"unexpected mlp.c_fc shape {W_fc.shape} vs {dst_fc.shape}\")\n",
    "        my.layers[i].fnn.linear1.bias.data.copy_(b_fc.to(my.layers[i].fnn.linear1.bias.device))\n",
    "\n",
    "        W_proj = hfsd[f\"transformer.h.{i}.mlp.c_proj.weight\"]\n",
    "        b_proj = hfsd[f\"transformer.h.{i}.mlp.c_proj.bias\"]\n",
    "        dst_proj = my.layers[i].fnn.linear2.weight\n",
    "        if tuple(W_proj.shape) == tuple(dst_proj.shape):\n",
    "            dst_proj.data.copy_(W_proj.to(dst_proj.device))\n",
    "        elif tuple(W_proj.t().contiguous().shape) == tuple(dst_proj.shape):\n",
    "            dst_proj.data.copy_(W_proj.t().contiguous().to(dst_proj.device))\n",
    "        else:\n",
    "            raise RuntimeError(f\"unexpected mlp.c_proj shape {W_proj.shape} vs {dst_proj.shape}\")\n",
    "        my.layers[i].fnn.linear2.bias.data.copy_(b_proj.to(my.layers[i].fnn.linear2.bias.device))\n",
    "print(\"Copy finished.\\n\")\n",
    "\n",
    "# ---------- parameter equality check ----------\n",
    "print(\"Checking per-layer parameter equality (q/k/v, c_proj, mlp) ...\")\n",
    "for i in range(cfg.n_layer):\n",
    "    # check q/k/v weights exactly equal\n",
    "    hW = hfsd[f\"transformer.h.{i}.attn.c_attn.weight\"]\n",
    "    if hW.shape[0] == cfg.n_embd and hW.shape[1] == 3*cfg.n_embd:\n",
    "        hWcanon = hW.t().contiguous()\n",
    "    elif hW.shape[0] == 3*cfg.n_embd and hW.shape[1] == cfg.n_embd:\n",
    "        hWcanon = hW\n",
    "    else:\n",
    "        raise RuntimeError(\"unexpected shape\")\n",
    "    Wq,Wk,Wv = hWcanon.split(cfg.n_embd, dim=0)\n",
    "    # convert to CPU for comparison\n",
    "    errors = []\n",
    "    def chk(name, hf_tensor, my_tensor):\n",
    "        hf = hf_tensor.detach().cpu()\n",
    "        myt = my_tensor.detach().cpu()\n",
    "        m = (hf-myt).abs().max().item()\n",
    "        if m != 0.0:\n",
    "            errors.append((name, m))\n",
    "    chk(f\"h{i}.q_w\", Wq, my.layers[i].self_attn.q_w.weight)\n",
    "    chk(f\"h{i}.k_w\", Wk, my.layers[i].self_attn.k_w.weight)\n",
    "    chk(f\"h{i}.v_w\", Wv, my.layers[i].self_attn.v_w.weight)\n",
    "    # c_proj\n",
    "    chk(f\"h{i}.c_proj\", hfsd[f\"transformer.h.{i}.attn.c_proj.weight\"], my.layers[i].self_attn.linear.weight)\n",
    "    # mlp\n",
    "    chk(f\"h{i}.c_fc\", hfsd[f\"transformer.h.{i}.mlp.c_fc.weight\"], my.layers[i].fnn.linear1.weight)\n",
    "    chk(f\"h{i}.c_proj_mlp\", hfsd[f\"transformer.h.{i}.mlp.c_proj.weight\"], my.layers[i].fnn.linear2.weight)\n",
    "    if errors:\n",
    "        print(\"Layer\", i, \"param mismatches (max abs):\", errors)\n",
    "        print(\"-> STOPPING: fix these first.\")\n",
    "        sys.exit(1)\n",
    "print(\"All layer params match exactly (max_abs == 0).\\n\")\n",
    "\n",
    "# ---------- create inputs and compute hf hidden states ----------\n",
    "from transformers import GPT2TokenizerFast\n",
    "tok = GPT2TokenizerFast.from_pretrained(HF_NAME)\n",
    "ids = tok(TEXT, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
    "attn_mask = torch.ones_like(ids, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    hf_out = hf(ids, attention_mask=attn_mask, output_hidden_states=True, return_dict=True)\n",
    "hf_hiddens = hf_out.hidden_states  # (embedding, after layer0, after layer1, ...)\n",
    "\n",
    "# ---------- 1) per-layer compare: apply my.layers[i] on hf_hiddens[i] (i.e. same input) ----------\n",
    "print(\"Per-layer single-apply compare (feed HF's layer-input into my layer) ...\")\n",
    "for i in range(cfg.n_layer):\n",
    "    hf_in = hf_hiddens[i].detach().clone()\n",
    "    my_block = my.layers[i]\n",
    "    hf_block = hf.transformer.h[i]\n",
    "    # apply my_layer to the HF input\n",
    "    with torch.no_grad():\n",
    "        my_out = my_block(hf_in, hf_in, hf_in, torch.ones_like(ids))\n",
    "    # compare with HF's post-layer output\n",
    "    hf_post = hf_hiddens[i+1].detach().clone()\n",
    "    s = maestat(hf_post, my_out)\n",
    "    print(f\"Layer {i}: MAE between hf_post and my_out = {s['mean_abs']:.6e}  max={s['max_abs']:.6e}\")\n",
    "    if s[\"mean_abs\"] != 0.0:\n",
    "        print(\">> Layer\", i, \"differs when applying my_block to HF input. Run detailed_block_debug on this layer.\")\n",
    "        # we stop early to do detailed debug next\n",
    "        break\n",
    "else:\n",
    "    print(\"All single-layer applications matched HF outputs exactly.\\n\")\n",
    "\n",
    "# ---------- 2) now do the full stack on my side (my.embedding -> my.layers sequential) ----------\n",
    "print(\"Running full stacked forward on my model and comparing per-layer outputs ...\")\n",
    "with torch.no_grad():\n",
    "    my_embed = my.embedding(ids)\n",
    "    h = my_embed\n",
    "    my_hiddens = []\n",
    "    # check if any in-place mutation occurs: keep copy of h before call and compare after\n",
    "    for i,layer in enumerate(my.layers):\n",
    "        # detect inplace: clone input\n",
    "        input_clone = h.detach().clone()\n",
    "        out = layer(h, h, h, torch.ones_like(ids))\n",
    "        # if the original input changed, that's in-place mutation\n",
    "        diff_inplace = (input_clone - h).abs().max().item()\n",
    "        if diff_inplace != 0.0:\n",
    "            print(f\"Layer {i}: detected in-place modification of input! max change = {diff_inplace}\")\n",
    "        my_hiddens.append(out)\n",
    "        h = out\n",
    "\n",
    "# compare stacked my_hiddens with hf_hiddens[1:]\n",
    "for i,(hf_h,my_h) in enumerate(zip(hf_hiddens[1:], my_hiddens)):\n",
    "    s = maestat(hf_h, my_h)\n",
    "    print(f\"Stacked Layer {i} hidden mae={s['mean_abs']:.6e} max={s['max_abs']:.6e}\")\n",
    "    if s[\"mean_abs\"] != 0.0:\n",
    "        print(\">> Difference appears at stacked layer\", i, \" (first mismatch).\")\n",
    "        # attempt a fine-grained check: feed HF input into my layer and compare (we already did that above)\n",
    "        break\n",
    "\n",
    "print(\"\\nDeep stack debug finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mynn()\n",
      "tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class mynn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hh = nn.Parameter(torch.ones((90,30)))\n",
    "\n",
    "aa = mynn()\n",
    "print(aa)\n",
    "print(torch.softmax(torch.ones((6)) * -1e-8, dim=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
